#!/usr/bin/env python3
"""
NSeekFS v1.0 - Comparative Benchmark
====================================

Compares NSeekFS with native NumPy/SciPy implementations
to demonstrate the advantages of the optimized Rust engine.

Usage:
    python benchmark_comparison.py [--datasets=small,medium,large] [--metrics=all] [--export=results.csv]
"""

import time
import numpy as np
import argparse
import csv
import json
import sys
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

try:
    from scipy.spatial.distance import cdist
    HAS_SCIPY = True
except ImportError:
    HAS_SCIPY = False
    print("‚ö†Ô∏è SciPy not found - some comparisons will be limited")

# Colors for output
class Colors:
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    BLUE = '\033[94m'
    MAGENTA = '\033[95m'
    CYAN = '\033[96m'
    BOLD = '\033[1m'
    END = '\033[0m'

def colored(text, color):
    return f"{color}{text}{Colors.END}"

def print_header(title):
    print("\n" + "="*70)
    print(colored(f"üöÄ {title}", Colors.BOLD + Colors.BLUE))
    print("="*70)

def print_result(message):
    print(f"{colored('üìä', Colors.CYAN)} {message}")

def print_winner(message):
    print(f"{colored('üèÜ', Colors.GREEN)} {message}")

def print_benchmark(name, time_ms, qps, extra=""):
    print(f"   {name:20s}: {time_ms:8.2f}ms  {qps:8.0f} QPS  {extra}")

class BenchmarkSuite:
    """Complete comparative benchmarking suite"""
    
    def __init__(self):
        self.results = []
        
    def numpy_cosine_similarity(self, query: np.ndarray, vectors: np.ndarray, top_k: int = 10) -> Tuple[np.ndarray, np.ndarray]:
        """NumPy implementation of cosine similarity"""
        # Normalize
        query_norm = query / np.linalg.norm(query)
        vectors_norm = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)
        
        # Compute similarities
        similarities = np.dot(vectors_norm, query_norm)
        
        # Top-k
        if top_k >= len(similarities):
            indices = np.argsort(similarities)[::-1]
        else:
            indices = np.argpartition(similarities, -top_k)[-top_k:]
            indices = indices[np.argsort(similarities[indices])][::-1]
        
        return indices, similarities[indices]
    
    def numpy_dot_product(self, query: np.ndarray, vectors: np.ndarray, top_k: int = 10) -> Tuple[np.ndarray, np.ndarray]:
        """NumPy implementation of dot product similarity"""
        # Compute dot products
        dot_products = np.dot(vectors, query)
        
        # Top-k (highest dot products)
        if top_k >= len(dot_products):
            indices = np.argsort(dot_products)[::-1]
        else:
            indices = np.argpartition(dot_products, -top_k)[-top_k:]
            indices = indices[np.argsort(dot_products[indices])][::-1]
        
        return indices, dot_products[indices]
    
    def numpy_euclidean_distance(self, query: np.ndarray, vectors: np.ndarray, top_k: int = 10) -> Tuple[np.ndarray, np.ndarray]:
        """NumPy implementation of Euclidean distance"""
        # Compute squared distances (faster than sqrt)
        diff = vectors - query[np.newaxis, :]
        distances = np.sum(diff * diff, axis=1)
        
        # Top-k (smallest distances)
        if top_k >= len(distances):
            indices = np.argsort(distances)
        else:
            indices = np.argpartition(distances, top_k-1)[:top_k]
            indices = indices[np.argsort(distances[indices])]
        
        return indices, distances[indices]
    
    def scipy_cosine_similarity(self, query: np.ndarray, vectors: np.ndarray, top_k: int = 10) -> Tuple[np.ndarray, np.ndarray]:
        """SciPy implementation of cosine similarity"""
        if not HAS_SCIPY:
            return self.numpy_cosine_similarity(query, vectors, top_k)
        
        distances = cdist([query], vectors, metric='cosine')[0]
        similarities = 1 - distances
        
        # Top-k
        if top_k >= len(similarities):
            indices = np.argsort(similarities)[::-1]
        else:
            indices = np.argpartition(similarities, -top_k)[-top_k:]
            indices = indices[np.argsort(similarities[indices])][::-1]
        
        return indices, similarities[indices]
    
    def scipy_euclidean_distance(self, query: np.ndarray, vectors: np.ndarray, top_k: int = 10) -> Tuple[np.ndarray, np.ndarray]:
        """SciPy implementation of Euclidean distance"""
        if not HAS_SCIPY:
            return self.numpy_euclidean_distance(query, vectors, top_k)
        
        distances = cdist([query], vectors, metric='euclidean')[0]
        
        # Top-k (smallest distances)
        if top_k >= len(distances):
            indices = np.argsort(distances)
        else:
            indices = np.argpartition(distances, top_k-1)[:top_k]
            indices = indices[np.argsort(distances[indices])]
        
        return indices, distances[indices]
    
    def benchmark_metric(self, vectors: np.ndarray, queries: List[np.ndarray], 
                        metric: str, top_k: int = 10) -> Dict[str, Any]:
        """Benchmark a specific metric - NSeekFS v1.0 API only supports: vectors, normalize, verbose"""
        
        print(f"\nüìç Benchmarking {metric.upper()} similarity...")
        print(f"   Dataset: {vectors.shape[0]:,} vectors √ó {vectors.shape[1]} dimensions")
        print(f"   Top-K: {top_k}, Trials: {len(queries)}")
        
        results = {}
        
        # 1. Test NSeekFS (using actual API signature from __init__.py)
        print(f"   üîß Testing NSeekFS...")
        try:
            import nseekfs
            
            # Create index - NSeekFS v1.0 API: from_embeddings(vectors, dims=None, normalize=True, verbose=False)
            index_start = time.time()
            # Only use parameters that exist in the real API
            index = nseekfs.from_embeddings(vectors, normalized=True, verbose=False)
            index_creation_time = time.time() - index_start
            
            # Test for all metrics since NSeekFS uses cosine internally anyway
            # Run queries
            nseekfs_times = []
            for query in queries:
                start_time = time.time()
                nseekfs_results = index.query(query, top_k=top_k)
                query_time = (time.time() - start_time) * 1000
                nseekfs_times.append(query_time)
            
            results['nseekfs'] = {
                'avg_time_ms': np.mean(nseekfs_times),
                'std_time_ms': np.std(nseekfs_times),
                'min_time_ms': np.min(nseekfs_times),
                'max_time_ms': np.max(nseekfs_times),
                'qps': 1000 / np.mean(nseekfs_times),
                'index_creation_time_s': index_creation_time,
                'simd_used': True,  # Assume SIMD for NSeekFS
                'metric_note': f'NSeekFS uses cosine similarity internally (tested with {metric} comparison)'
            }
            
            print_benchmark("NSeekFS", 
                          results['nseekfs']['avg_time_ms'],
                          results['nseekfs']['qps'],
                          f"¬±{results['nseekfs']['std_time_ms']:.1f}")
            
        except Exception as e:
            print(f"   ‚ùå NSeekFS failed: {e}")
            results['nseekfs'] = {'error': str(e)}
        
        # 2. Test NumPy
        print(f"   üî¢ Testing NumPy...")
        try:
            numpy_times = []
            
            if metric == 'cosine':
                func = self.numpy_cosine_similarity
            elif metric == 'dot':
                func = self.numpy_dot_product
            elif metric == 'euclidean':
                func = self.numpy_euclidean_distance
            else:
                raise ValueError(f"Unsupported metric: {metric}")
            
            for query in queries:
                start_time = time.time()
                indices, scores = func(query, vectors, top_k)
                query_time = (time.time() - start_time) * 1000
                numpy_times.append(query_time)
            
            results['numpy'] = {
                'avg_time_ms': np.mean(numpy_times),
                'std_time_ms': np.std(numpy_times),
                'min_time_ms': np.min(numpy_times),
                'max_time_ms': np.max(numpy_times),
                'qps': 1000 / np.mean(numpy_times),
                'simd_used': False
            }
            
            print_benchmark("NumPy", 
                          results['numpy']['avg_time_ms'],
                          results['numpy']['qps'],
                          f"¬±{results['numpy']['std_time_ms']:.1f}")
            
        except Exception as e:
            print(f"   ‚ùå NumPy failed: {e}")
            results['numpy'] = {'error': str(e)}
        
        # 3. Test SciPy (if available)
        if HAS_SCIPY and metric in ['cosine', 'euclidean']:
            print(f"   üî¨ Testing SciPy...")
            try:
                scipy_times = []
                
                if metric == 'cosine':
                    func = self.scipy_cosine_similarity
                elif metric == 'euclidean':
                    func = self.scipy_euclidean_distance
                
                for query in queries:
                    start_time = time.time()
                    indices, scores = func(query, vectors, top_k)
                    query_time = (time.time() - start_time) * 1000
                    scipy_times.append(query_time)
                
                results['scipy'] = {
                    'avg_time_ms': np.mean(scipy_times),
                    'std_time_ms': np.std(scipy_times),
                    'min_time_ms': np.min(scipy_times),
                    'max_time_ms': np.max(scipy_times),
                    'qps': 1000 / np.mean(scipy_times),
                    'simd_used': False
                }
                
                print_benchmark("SciPy", 
                              results['scipy']['avg_time_ms'],
                              results['scipy']['qps'],
                              f"¬±{results['scipy']['std_time_ms']:.1f}")
                
            except Exception as e:
                print(f"   ‚ùå SciPy failed: {e}")
                results['scipy'] = {'error': str(e)}
        
        # Analysis for this metric
        print(f"\n   üìä Analysis for {metric}:")
        
        # Determine winner
        valid_results = {k: v for k, v in results.items() if 'avg_time_ms' in v}
        
        if valid_results:
            fastest = min(valid_results.keys(), key=lambda x: valid_results[x]['avg_time_ms'])
            fastest_time = valid_results[fastest]['avg_time_ms']
            
            print(f"   üèÜ Fastest: {fastest.upper()} ({fastest_time:.2f}ms)")
            
            # Calculate speedups
            if 'nseekfs' in valid_results:
                nseekfs_time = valid_results['nseekfs']['avg_time_ms']
                for impl, data in valid_results.items():
                    if impl != 'nseekfs':
                        speedup = data['avg_time_ms'] / nseekfs_time
                        if speedup > 1:
                            print(f"   ‚ö° NSeekFS is {speedup:.1f}x faster than {impl.upper()}")
                        else:
                            print(f"   üìà {impl.upper()} is {1/speedup:.1f}x faster than NSeekFS")
            elif 'nseekfs' in results and 'skipped' in results['nseekfs']:
                print(f"   ‚ÑπÔ∏è NSeekFS: {results['nseekfs']['skipped']}")
            elif 'nseekfs' in results and 'metric_note' in results['nseekfs']:
                print(f"   ‚ÑπÔ∏è Note: {results['nseekfs']['metric_note']}")
        else:
            print("   ‚ùå No valid results for comparison")
        
        return results
    
    def benchmark_dataset(self, dataset_name: str, n_vectors: int, dimensions: int, 
                         num_queries: int = 5, metrics: List[str] = None, 
                         top_k: int = 10) -> Dict[str, Any]:
        """Complete benchmark of a dataset"""
        
        print_header(f"DATASET: {dataset_name.upper()}")
        
        if metrics is None:
            metrics = ['cosine', 'dot', 'euclidean']  # Test all metrics, but NSeekFS only optimized for cosine
        
        # Generate data
        print_result(f"Generating {dataset_name} dataset: {n_vectors:,} √ó {dimensions}D...")
        np.random.seed(42)  # Reproducibility
        vectors = np.random.randn(n_vectors, dimensions).astype(np.float32)
        queries = [np.random.randn(dimensions).astype(np.float32) for _ in range(num_queries)]
        
        data_size_mb = vectors.nbytes / (1024 * 1024)
        print_result(f"   ‚úÖ Generated {dataset_name}: {data_size_mb:.1f}MB")
        
        results = {
            'dataset_name': dataset_name,
            'n_vectors': n_vectors,
            'dimensions': dimensions,
            'num_queries': num_queries,
            'top_k': top_k,
            'data_size_mb': data_size_mb,
            'metrics': {}
        }
        
        # Benchmark for each metric
        for metric in metrics:
            print_result(f"\nüìç Benchmarking {metric.upper()} similarity...")
            metric_results = self.benchmark_metric(vectors, queries, metric, top_k)
            results['metrics'][metric] = metric_results
        
        return results
    
    def run_comprehensive_benchmark(self, datasets: List[str] = None, 
                                  metrics: List[str] = None, 
                                  top_k: int = 10) -> List[Dict[str, Any]]:
        """Run comprehensive benchmark suite"""
        
        if datasets is None:
            datasets = ['small', 'medium', 'large']
        
        if metrics is None:
            metrics = ['cosine', 'dot', 'euclidean']  # Test all metrics
        
        # Dataset configurations
        dataset_configs = {
            'tiny': {'n_vectors': 1000, 'dimensions': 128, 'num_queries': 10},
            'small': {'n_vectors': 5000, 'dimensions': 256, 'num_queries': 20},
            'medium': {'n_vectors': 25000, 'dimensions': 384, 'num_queries': 10},
            'large': {'n_vectors': 100000, 'dimensions': 512, 'num_queries': 5},
            'xlarge': {'n_vectors': 250000, 'dimensions': 768, 'num_queries': 3},
            'extreme': {'n_vectors': 500000, 'dimensions': 1024, 'num_queries': 2}
        }
        
        all_results = []
        
        print_header("üöÄ COMPREHENSIVE BENCHMARK SUITE")
        print_result(f"Testing datasets: {', '.join(datasets)}")
        print_result(f"Testing metrics: {', '.join(metrics)}")
        print_result(f"Top-K: {top_k}")
        
        for dataset_name in datasets:
            if dataset_name not in dataset_configs:
                print(f"‚ö†Ô∏è Unknown dataset: {dataset_name}")
                continue
                
            config = dataset_configs[dataset_name]
            
            try:
                result = self.benchmark_dataset(
                    dataset_name=dataset_name,
                    n_vectors=config['n_vectors'],
                    dimensions=config['dimensions'],
                    num_queries=config['num_queries'],
                    metrics=metrics,
                    top_k=top_k
                )
                all_results.append(result)
                self.results.append(result)
                
            except Exception as e:
                print(f"‚ùå Error in dataset {dataset_name}: {e}")
                import traceback
                traceback.print_exc()
        
        # Generate final analysis
        self.print_final_analysis()
        
        return all_results
    
    def print_final_analysis(self) -> None:
        """Generate summary report of benchmarks"""
        
        print_header("üìà FINAL ANALYSIS")
        
        if not self.results:
            print("‚ùå No results available")
            return
        
        # General analysis
        total_tests = sum(len(r['metrics']) for r in self.results)
        print_result(f"Total tests executed: {total_tests}")
        print_result(f"Datasets tested: {len(self.results)}")
        
        # Analysis by metric
        metrics_analysis = {}
        
        for result in self.results:
            for metric, metric_data in result['metrics'].items():
                if metric not in metrics_analysis:
                    metrics_analysis[metric] = {
                        'nseekfs_wins': 0,
                        'numpy_wins': 0,
                        'scipy_wins': 0,
                        'total_tests': 0,
                        'speedups': []
                    }
                
                # Determine winner
                times = {}
                for impl in ['nseekfs', 'numpy', 'scipy']:
                    if impl in metric_data and isinstance(metric_data[impl], dict) and 'avg_time_ms' in metric_data[impl]:
                        times[impl] = metric_data[impl]['avg_time_ms']
                
                if times:
                    winner = min(times.keys(), key=lambda x: times[x])
                    metrics_analysis[metric][f'{winner}_wins'] += 1
                    metrics_analysis[metric]['total_tests'] += 1
                    
                    # Calculate NSeekFS speedup vs best alternative
                    if 'nseekfs' in times:
                        nseekfs_time = times['nseekfs']
                        other_times = [t for k, t in times.items() if k != 'nseekfs']
                        if other_times:
                            best_other_time = min(other_times)
                            speedup = best_other_time / nseekfs_time
                            metrics_analysis[metric]['speedups'].append(speedup)
        
        # Print metric analysis
        for metric, analysis in metrics_analysis.items():
            print_result(f"\nüéØ {metric.upper()} Similarity Results:")
            print(f"   NSeekFS wins: {analysis['nseekfs_wins']}/{analysis['total_tests']}")
            print(f"   NumPy wins: {analysis['numpy_wins']}/{analysis['total_tests']}")
            print(f"   SciPy wins: {analysis['scipy_wins']}/{analysis['total_tests']}")
            
            if analysis['speedups']:
                avg_speedup = np.mean(analysis['speedups'])
                max_speedup = np.max(analysis['speedups'])
                print(f"   Average NSeekFS speedup: {avg_speedup:.1f}x")
                print(f"   Maximum NSeekFS speedup: {max_speedup:.1f}x")
        
        # Overall conclusion
        total_nseekfs_wins = sum(a['nseekfs_wins'] for a in metrics_analysis.values())
        total_benchmarks = sum(a['total_tests'] for a in metrics_analysis.values())
        
        if total_benchmarks > 0:
            win_rate = (total_nseekfs_wins / total_benchmarks) * 100
            
            print_header("üèÜ OVERALL CONCLUSION")
            print_result(f"NSeekFS win rate: {win_rate:.1f}% ({total_nseekfs_wins}/{total_benchmarks})")
            
            if win_rate >= 80:
                print_winner("üöÄ NSeekFS consistently outperforms alternatives!")
            elif win_rate >= 60:
                print_winner("‚ö° NSeekFS shows strong performance advantages")
            elif win_rate >= 40:
                print_result("üìä NSeekFS shows competitive performance")
            else:
                print_result("üìà NSeekFS has room for optimization")
        
        print_result(f"\nüí° Key Features of NSeekFS v1.0:")
        print_result("   ‚Ä¢ Rust-powered performance with SIMD optimizations")
        print_result("   ‚Ä¢ Exact cosine similarity search (100% precision)")
        print_result("   ‚Ä¢ Memory-efficient index structures")
        print_result("   ‚Ä¢ Thread-safe concurrent access")
        print_result("   ‚Ä¢ Production-ready with robust error handling")
        print_result("   ‚Ä¢ Future versions will support additional metrics")

def export_results(results: List[Dict[str, Any]], filename: str) -> None:
    """Export benchmark results to file"""
    
    file_ext = filename.lower().split('.')[-1]
    
    if file_ext == 'json':
        with open(filename, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        print_result(f"Results exported to {filename}")
    
    elif file_ext == 'csv':
        # Flatten results for CSV
        flattened_results = []
        
        for result in results:
            base_info = {
                'dataset_name': result['dataset_name'],
                'n_vectors': result['n_vectors'],
                'dimensions': result['dimensions'],
                'data_size_mb': result['data_size_mb']
            }
            
            for metric, metric_data in result['metrics'].items():
                for impl, impl_data in metric_data.items():
                    if isinstance(impl_data, dict) and 'avg_time_ms' in impl_data:
                        row = {
                            **base_info,
                            'metric': metric,
                            'implementation': impl,
                            'avg_time_ms': impl_data['avg_time_ms'],
                            'qps': impl_data['qps'],
                            'std_time_ms': impl_data.get('std_time_ms', 0)
                        }
                        flattened_results.append(row)
        
        if flattened_results:
            with open(filename, 'w', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=flattened_results[0].keys())
                writer.writeheader()
                writer.writerows(flattened_results)
            print_result(f"Results exported to {filename}")
        else:
            print("‚ùå No results to export")
    
    else:
        print(f"‚ùå Unsupported export format: {file_ext}. Use .json or .csv")

def run_quick_comparison():
    """Quick comparison for testing"""
    print_header("QUICK COMPARISON")
    
    try:
        import nseekfs
        
        # Small dataset for quick test
        vectors = np.random.randn(5000, 256).astype(np.float32)
        query = np.random.randn(256).astype(np.float32)
        
        print_result(f"Dataset: {vectors.shape[0]:,} vectors √ó {vectors.shape[1]}D")
        
        # NSeekFS (using real API signature)
        print_result("Testing NSeekFS...")
        start_time = time.time()
        index = nseekfs.from_embeddings(vectors, normalized=True, verbose=False)
        build_time = time.time() - start_time
        
        start_time = time.time()
        results = index.query(query, top_k=10)
        nseekfs_time = (time.time() - start_time) * 1000
        
        # NumPy
        print_result("Testing NumPy...")
        query_norm = query / np.linalg.norm(query)
        vectors_norm = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)
        
        start_time = time.time()
        similarities = np.dot(vectors_norm, query_norm)
        top_indices = np.argpartition(similarities, -10)[-10:]
        numpy_time = (time.time() - start_time) * 1000
        
        # Results
        print_result(f"\nüèÜ Results:")
        print_benchmark("NSeekFS", nseekfs_time, 1000/nseekfs_time)
        print_benchmark("NumPy", numpy_time, 1000/numpy_time)
        
        speedup = numpy_time / nseekfs_time
        if speedup > 1:
            print_winner(f"üöÄ NSeekFS is {speedup:.1f}x faster!")
        else:
            print_result(f"NumPy is {1/speedup:.1f}x faster")
        
        print_result(f"üíæ NSeekFS memory usage: {index.memory_usage_mb:.1f}MB")
        
    except ImportError:
        print("‚ùå NSeekFS not found")
    except Exception as e:
        print(f"‚ùå Error: {e}")

def main():
    """Main function"""
    
    parser = argparse.ArgumentParser(description='NSeekFS v1.0 Comparative Benchmark')
    parser.add_argument('--datasets', type=str, default='small,medium',
                       help='Datasets to test (tiny,small,medium,large,xlarge,extreme)')
    parser.add_argument('--metrics', type=str, default='cosine,dot,euclidean',
                       help='Metrics to test (cosine,dot,euclidean)')
    parser.add_argument('--top-k', type=int, default=10, help='Number of top-k results')
    parser.add_argument('--export', type=str, help='Export results (filename.json or filename.csv)')
    parser.add_argument('--quick', action='store_true', help='Quick comparison only')
    
    args = parser.parse_args()
    
    print(colored("üöÄ NSEEKFS v1.0 - COMPREHENSIVE BENCHMARK", Colors.BOLD + Colors.BLUE))
    print(colored("=" * 70, Colors.BLUE))
    
    if args.quick:
        run_quick_comparison()
        return 0
    
    # Parse arguments
    datasets = [d.strip() for d in args.datasets.split(',')]
    metrics = [m.strip() for m in args.metrics.split(',')]
    
    # Check dependencies
    try:
        import nseekfs
        print_result(f"‚úÖ NSeekFS v{getattr(nseekfs, '__version__', '1.0.0')} available")
    except ImportError:
        print("‚ùå NSeekFS not found. Install with: pip install nseekfs")
        return 1
    
    print_result(f"‚úÖ NumPy {np.__version__} available")
    
    if HAS_SCIPY:
        import scipy
        print_result(f"‚úÖ SciPy {scipy.__version__} available")
    else:
        print_result("‚ö†Ô∏è SciPy not available - some comparisons limited")
    
    # Run benchmarks
    try:
        suite = BenchmarkSuite()
        
        start_time = time.time()
        results = suite.run_comprehensive_benchmark(datasets=datasets, metrics=metrics, top_k=args.top_k)
        total_time = time.time() - start_time
        
        print_result(f"\n‚è±Ô∏è Total benchmark time: {total_time:.1f}s")
        
        # Export if requested
        if args.export:
            export_results(results, args.export)
        
        print_result(f"\n‚úÖ Comparative benchmark completed!")
        return 0
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Benchmark interrupted by user")
        return 2
    except Exception as e:
        print(f"\n‚ùå Benchmark error: {e}")
        import traceback
        traceback.print_exc()
        return 3

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)